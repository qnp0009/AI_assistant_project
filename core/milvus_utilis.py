from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection
from core.embedding import embed_chunks, split_into_chunks
import uuid
import time

print("üïí Connecting to Milvus...")
start_time = time.time()

# 1. K·∫øt n·ªëi t·ªõi Milvus
connections.connect(host="localhost", port="19530")

connect_time = time.time() - start_time
print(f"‚úÖ Connected to Milvus in {connect_time:.2f} seconds")

# 2. ƒê·ªãnh nghƒ©a schema
fields = [
    FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
    FieldSchema(name="filename", dtype=DataType.VARCHAR, max_length=200),
    FieldSchema(name="chunk", dtype=DataType.VARCHAR, max_length=1000),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
]
schema = CollectionSchema(fields, description="Document Chunks")

# 3. T·∫°o ho·∫∑c l·∫•y Collection
collection = Collection("documents", schema=schema)

if not collection.has_index():  # Ki·ªÉm tra ƒë√£ c√≥ index ch∆∞a
    print("üïí Creating index...")
    index_start = time.time()
    collection.create_index(
        field_name="embedding",
        index_params={
            "index_type": "IVF_SQ8",  # More efficient than IVF_FLAT
            "metric_type": "IP",     # Inner Product (c√†ng g·∫ßn c√†ng t·ªët)
            "params": {"nlist": 1024}  # Increased for better accuracy/speed balance
        }
    )
    index_time = time.time() - index_start
    print(f"‚úÖ Index created in {index_time:.2f} seconds")

def save_to_milvus(chunks: list[str], filename: str, vectors: list[list[float]] | None = None):
    """
    L∆∞u c√°c ƒëo·∫°n vƒÉn v√† vector t∆∞∆°ng ·ª©ng v√†o Milvus.
    M·ªói ƒëo·∫°n c√≥ 1 id ng·∫´u nhi√™n.
    """
    start_time = time.time()
    
    # Generate embeddings if not provided
    if vectors is None:
        vectors = embed_chunks(chunks)
    
    # Generate unique IDs
    ids = [str(uuid.uuid4()) for _ in chunks]

    # Get filename
    filenames = [filename]*len(chunks)

    # Insert into Milvus
    collection.insert([ids, filenames, chunks, vectors])
    # Only flush if we have a small number of chunks
    if len(chunks) <= 10:
        collection.flush()
    
    save_time = time.time() - start_time
    print(f"‚úÖ Saved {len(chunks)} chunks to Milvus in {save_time:.2f} seconds")

def search_similar_chunks(query: str, top_k: int = 1000):
    """
    Nh√∫ng c√¢u h·ªèi, t√¨m top_k ƒëo·∫°n vƒÉn g·∫ßn nh·∫•t trong Milvus
    """
    start_time = time.time()
    
    # Get query embedding - pass the query as a single-item list
    query_vectors = embed_chunks([query])
    if not query_vectors:
        raise ValueError("Failed to generate embedding for query")
    
    query_vector = query_vectors[0]  # Get first (and only) vector
    collection.load()  # ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë√£ s·∫µn s√†ng

    results = collection.search(
        data=[query_vector],
        anns_field="embedding",
        param={"metric_type": "IP", "params": {"nprobe": 32}},  # Increased nprobe for better accuracy
        limit=top_k,
        output_fields=["chunk"]
    )

    matches = []
    for hit in results[0]:
        matches.append({
            "score": hit.score,
            "chunk": hit.entity.get("chunk")
        })

    search_time = time.time() - start_time
    print(f"‚è±Ô∏è Search completed in {search_time:.2f} seconds")
    return matches
    
def delete_file(filename: str):
    collection.load()
    collection.delete(expr=f'filename == "{filename}"')
    collection.flush()
    print(f"‚úÖ ƒê√£ x√≥a to√†n b·ªô chunks c·ªßa {filename} kh·ªèi Milvus.")

    return {
        "filename":filename,
        "message": f"‚úÖ ƒê√£ x√≥a to√†n b·ªô chunks c·ªßa {filename} kh·ªèi Milvus."
    }